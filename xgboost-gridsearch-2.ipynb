{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import essential packages.\n"
     ]
    }
   ],
   "source": [
    "print('Import essential packages.')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys, os\n",
    "import math\n",
    "from IPython.utils import io\n",
    "import copy\n",
    "\n",
    "# pd.set_option('display.max_columns', 1000)\n",
    "# pd.set_option('display.max_rows', 92000)\n",
    "# pd.set_option('max_info_columns', 91713)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    '''create a dataframe and optimize its memory usage'''\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col = 'encounter_id')\n",
    "    #     df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def hospital_source_observation(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe without observation and update it with the most logical guess.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #Fill missing values in the hospital admit source with ICU admit source\n",
    "    df_hos_NA = df[df['hospital_admit_source'].isna()==True]    \n",
    "    \n",
    "    #for the rows with missing value in the training data set's hospital admit source\n",
    "    for i in df_hos_NA.index:\n",
    "        \n",
    "        #if the icu amdit source is Accident & Emergency\n",
    "        if df.loc[i,'icu_admit_source']=='Accident & Emergency':\n",
    "            #hospital admit source is filled with Emergency Department\n",
    "            df.loc[i,'hospital_admit_source'] = 'Emergency Department'\n",
    "        \n",
    "        #if the icu admit source is Operating Room/Recovery\n",
    "        elif df.loc[i,'icu_admit_source'] == 'Operating Room / Recovery':\n",
    "            #if there is post operation, fill it with Operating Room\n",
    "            if df.loc[i,'apache_post_operative'] == 1:\n",
    "                df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "            #if not, fill it with Recovery Room\n",
    "            else:\n",
    "                df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "        #Fill the rest of the hospital admit source with ICU admit source\n",
    "        else:\n",
    "            df.loc[i,'hospital_admit_source'] = df.loc[i,'icu_admit_source']\n",
    "                \n",
    "    observation = df[df['hospital_admit_source']=='Observation']\n",
    "    \n",
    "    for i in observation.index:\n",
    "        \n",
    "        if df.loc[i,'icu_admit_source']!='Floor':\n",
    "            if df.loc[i,'icu_admit_source']=='Operating Room / Recovery':\n",
    "                if df.loc[i,'apache_post_operative'] == 0:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "                else:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "         \n",
    "            else:\n",
    "                df['icu_type'] = df['icu_type'].str.lower()\n",
    "                if df.loc[i,'icu_type'].find('s')!= -1:\n",
    "                    if df.loc[i,'apache_post_operative'] == 0:\n",
    "                        df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "                    else:\n",
    "                        df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "                else:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Emergency Department'\n",
    "                        \n",
    "        else:\n",
    "            df.loc[i,'hospital_admit_source'] = df.loc[i,'icu_admit_source']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def assess_NA(data):\n",
    "    \"\"\"\n",
    "    Returns a dataframe denoting the total number of NA values and the percentage of NA values in each column.\n",
    "    The column names are notes on the index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    \"\"\"\n",
    "    #pandas series denoting features and the sum of their null values\n",
    "    null_sum = data.isnull().sum()\n",
    "    \n",
    "    #instantiate columns for missing data\n",
    "    total = null_sum\n",
    "    percent = (((null_sum/len(data.index))*100).round(2))\n",
    "    \n",
    "    #concatenate along the columns to create the complete dataframe\n",
    "    train_NA = pd.concat([total,percent],axis=1,keys=['Number of NA','Percent NA'])\n",
    "    \n",
    "    #drop rows that don't have any missing data\n",
    "    train_NA = train_NA[(train_NA.T !=0).any()].sort_values(ascending=False, by='Percent NA')\n",
    "    \n",
    "    return train_NA\n",
    "\n",
    "def data_processing(df, train, change_to_bool, change_to_cat, excludeColumns, tag):\n",
    "    \n",
    "    ## Drop outcome variable\n",
    "    #     if tag == 'train':\n",
    "    #         df = df.drop(columns=['hospital_death'])\n",
    "                \n",
    "    '''Revise certain Variables\n",
    "    '''\n",
    "    print('- Revise hospital_admit_source, and apache_2_bodysystem')#pre_icu_los_days\n",
    "    # Modify negative value to positive of pre_icu_los_days\n",
    "    # df['pre_icu_los_days'] = abs(df['pre_icu_los_days'])\n",
    "    # Rename some categorical variable's list name\n",
    "    df['hospital_admit_source'] = df['hospital_admit_source'].replace(\n",
    "        {'Other ICU': 'ICU', \n",
    "         'ICU to SDU':'SDU', \n",
    "         'Step-Down Unit (SDU)': 'SDU',\n",
    "         'Other':'Other Hospital', \n",
    "         'Observation': 'Recovery Room',\n",
    "         'Acute Care/Floor': 'Acute Care'})\n",
    "    df['apache_2_bodysystem'] = df['apache_2_bodysystem'].replace(\n",
    "        {'Undefined diagnoses': 'Undefined Diagnoses'}\n",
    "    )\n",
    "    # print('- Change -1 to nan for apache_4a_hospital_death_prob and apache_4a_icu_death_prob')\n",
    "    # Modify negative value of apache_4a_hospital_death_prob to nan\n",
    "    #     df.loc[df['apache_4a_hospital_death_prob'] == -1, 'apache_4a_hospital_death_prob'] = np.nan\n",
    "    #     df.loc[df['apache_4a_icu_death_prob'] == -1, 'apache_4a_icu_death_prob'] = np.nan\n",
    "    \n",
    "    ### Deal with d1, h1\n",
    "    new_d1_h1_cols = []\n",
    "    \n",
    "    ## Method 0\n",
    "    #print('- Remain h1_d1 as original value')\n",
    "    \n",
    "    ## method 1\n",
    "    ## Change all d1/h1 into booline by assigning 1 to those with values and 0 to those with NaN\n",
    "    #     d1_cols = [col for col in df.columns if (col.find('d1_') != -1)]\n",
    "    #     h1_cols = [col for col in df.columns if (col.find('h1_') != -1)]    \n",
    "    #     for col in d1_cols:\n",
    "    #         df[col] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    #     for col in h1_cols:\n",
    "    #         df[col] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    \n",
    "    ## method 1-2\n",
    "    ## Creat booline columns for d1/h1 by assigning 0 to those with values and 1 to those with NaN\n",
    "    #     print('- Creat booline columns for d1/h1')\n",
    "    #     d1_cols = [col for col in df.columns if (col.find('d1_') != -1)]\n",
    "    #     h1_cols = [col for col in df.columns if (col.find('h1_') != -1)]    \n",
    "    #     for col in d1_cols:\n",
    "    #         df[col+'_bool'] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    #     for col in h1_cols:\n",
    "    #         df[col+'_bool'] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    \n",
    "    ## method 2\n",
    "    #     print('- Create columns of combination of h1_d1_min_max: ')\n",
    "    #     # Add new features by aggrigating d1/h1/and their min/max\n",
    "    #     d1_cols_min = [col for col in df.columns \n",
    "    #                if (col.find('d1_') != -1 and col.find('_min') != -1)]\n",
    "    #     d1_cols_max = [col for col in df.columns \n",
    "    #                if (col.find('d1_') != -1 and col.find('_max') != -1)]\n",
    "    #     h1_cols_max = [col for col in df.columns \n",
    "    #            if (col.find('h1_') != -1 and col.find('_max') != -1)]\n",
    "\n",
    "    #     for d1_min in d1_cols_min:\n",
    "    #         d1_max = d1_min.replace('_min', '_max')\n",
    "    #         h1_min = d1_min.replace('d1', 'h1')\n",
    "    #         h1_max = h1_min.replace('_min', '_max')\n",
    "\n",
    "    #         df['range_' + d1_min.strip('_min')] = df[d1_max] - df[d1_min]; new_d1_h1_cols.append(df['range_' + d1_min.strip('_min')])\n",
    "    #         df['range_' + h1_min.strip('_min')] = df[h1_max] - df[h1_min]; new_d1_h1_cols.append(df['range_' + h1_min.strip('_min')])\n",
    "\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMin'] = df[d1_min] - df[h1_min]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMin'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMax'] = df[d1_min] - df[h1_max]; new_d1_h1_cols.append( df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMax'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMin'] = df[d1_max] - df[h1_min]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMin'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMax'] = df[d1_max] - df[h1_max]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMax'])\n",
    "\n",
    "    #     print('  There are %d columns adding in' % (len(new_d1_h1_cols)))    \n",
    "\n",
    "    #     ## Record the max columns for all d1 and h1 for future dropping list\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns = set(excludeColumns).union(d1_cols_max).union(h1_cols_max)\n",
    "\n",
    "    ### hospital_id & icu_id\n",
    "    ## Method 1\n",
    "    #     print('- Include hospital_id and icu_id and remain as original value')\n",
    "    ## Method 2\n",
    "    print('- Transfer icu_id and hopstipal_id to categorical data')\n",
    "    if tag=='train':\n",
    "        change_to_cat.append('icu_id')\n",
    "        change_to_cat.append('hospital_id')\n",
    "    ## Method 2.2\n",
    "    #     print('Transfer icu_id and to categorical data and drop hopstipal_id')\n",
    "    #     change_to_cat.append('icu_id')\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('apache_3j_diagnosis')\n",
    "    ## Method 3\n",
    "    #     print('- Exclude columns patient_id, icu_id, hospital_id, and apache_3j_diagnosis')\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('icu_id')\n",
    "    #         excludeColumns.add('hospital_id')\n",
    "    #         excludeColumns.add('patient_id')\n",
    "    #         excludeColumns.add('apache_3j_diagnosis')\n",
    "    \n",
    "\n",
    "    ''' Drop certain columns by some criteria\n",
    "    '''\n",
    "    ### Exclude collinear (highly correlated) features \n",
    "    ## by dropping columns with correlations above threshold\n",
    "    if tag == 'train':\n",
    "        threshold = 0.9 # Threshold for removing correlated variables\n",
    "        corr_matrix = df.drop(columns='hospital_death').corr().abs() # Absolute value correlation matrix\n",
    "        # corr_matrix = df.corr().abs() # Absolute value correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) # Upper triangle of correlations\n",
    "        to_drop = set( [column for column in upper.columns if any(upper[column] > threshold)] )\n",
    "        print('- There are %d columns to remove due to high correlation with threshold=0.9.' % (len(to_drop)))\n",
    "\n",
    "        # record these columns into the list, excludeColumns\n",
    "        excludeColumns = excludeColumns.union(to_drop)\n",
    "\n",
    "#         ## Exclude h1_xxx_max and d1_xxx_max features \n",
    "#         df = df[df.columns.drop(list(df.filter(regex='_max')))]\n",
    "    \n",
    "    \n",
    "    '''Deal with missing values\n",
    "    '''\n",
    "    ### fillna in height/weight/bmi\n",
    "    if tag=='train':\n",
    "        print('- Fill NaN of age, height, weight, and bmi')\n",
    "        df['age'] = df['age'].fillna(df['age'].median())\n",
    "        df['height'] = df.groupby(['ethnicity', 'gender'])['height'].apply(lambda x: x.fillna(x.median()))  \n",
    "        df['weight'] = df.groupby(['ethnicity', 'gender', 'height'])['weight'].apply(lambda x: x.fillna(x.median()))    \n",
    "        # print('Drop height and weight, remain only column bmi ')    \n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('height')\n",
    "    #         excludeColumns.add('weight')\n",
    "    else:\n",
    "        print('- Fill NaN of age, height, weight, and bmi by values from training data set')\n",
    "        df['age'] = df['age'].fillna(train['age'].median())\n",
    "        df['height'] = df['height'].fillna(train.groupby(['ethnicity', 'gender'])['height'].transform('median'))\n",
    "        df['weight'] = df['weight'].fillna(train.groupby(['ethnicity', 'gender', 'height'])['weight'].transform('median'))\n",
    "    \n",
    "    df['bmi'] = df.apply( lambda x: x['weight']/(0.01 * x['height'])**2 if np.isnan(x['bmi']) else x['bmi'], axis=1)\n",
    "    \n",
    "    ### fillna for hospital_admit_source\n",
    "    print('- Fill NaN of hospital_admit_source by using icu_type')\n",
    "    df = hospital_source_observation(df)\n",
    "\n",
    "    print('- Fill NaN of hospital_admit_source by using most frequent value in icu_type')    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "    imputer.fit(df['hospital_admit_source'].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    if tag == 'train':\n",
    "        df['hospital_admit_source'] = imputer.transform(df['hospital_admit_source'].values.reshape(-1, 1))\n",
    "    else:\n",
    "        most_freq = train['hospital_admit_source'].value_counts().keys()[0]\n",
    "        df['hospital_admit_source'] = df['hospital_admit_source'].fillna(most_freq)\n",
    "\n",
    "    ### fillna for apache_4a_hospital_death_prob and apache_4a_icu_death_prob\n",
    "    print('- Fill NaN of apache_4a_hospital_death_prob and apache_4a_icu_death_prob by each other')\n",
    "    df['apache_4a_hospital_death_prob'\n",
    "        ] = pd.DataFrame(df.loc[:,['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']\n",
    "                                 ].apply(lambda x: x['apache_4a_icu_death_prob'] if np.isnan(x['apache_4a_hospital_death_prob']) \\\n",
    "                                                                                 else x['apache_4a_hospital_death_prob'], \n",
    "                                         axis=1))\n",
    "    df['apache_4a_icu_death_prob'\n",
    "        ] = pd.DataFrame(df.loc[:,['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']\n",
    "                                 ].apply(lambda x: x['apache_4a_hospital_death_prob'] if np.isnan(x['apache_4a_icu_death_prob']) \\\n",
    "                                                                                      else x['apache_4a_icu_death_prob'], \n",
    "                                         axis=1))\n",
    "\n",
    "    ### fillna for all numerical and object variables using '9999'\n",
    "    #     print('- Fill NaN as \"9999\" for the following object columns:')\n",
    "    #     i = 0\n",
    "    #     for col in change_to_cat:\n",
    "    #         if i==0:\n",
    "    #             print(\"=> [\"+col, end=', ')\n",
    "    #         elif i!=len(change_to_cat)-1:\n",
    "    #             print(col, end=', ')\n",
    "    #         else:\n",
    "    #             print(col+']')\n",
    "    #         i+=1\n",
    "    #     for col in change_to_cat:\n",
    "    #         if col != 'age':\n",
    "    #             df[col] = df[col].fillna('9999')\n",
    "    #             df[col] = df[col].astype('str')\n",
    "\n",
    "    #     print('- Fill NaN as \"9999\" for the remaining numeric columns')\n",
    "    #     for col in set(df.columns).difference(set(change_to_cat)):\n",
    "    #         df[col] = df[col].fillna(9999)\n",
    "    \n",
    "    ### Binary variables\n",
    "    ## Method 1: make NaN as a new group NaNN: \n",
    "    # which represents a patient either doesn't need to fill the form,\n",
    "    # or they couldn't fill the form\n",
    "    # print('Create an NANN category for boolean variables')\n",
    "    # for col in change_to_bool:\n",
    "    #     df[col] = df[col].fillna('NANN')\n",
    "    #     df[col] = df[col].astype('category')\n",
    "    ## Method 2: change NaN --> 0\n",
    "    #     for col in change_to_bool:\n",
    "    #         df[col] = df[col].fillna(0)\n",
    "    \n",
    "    ## change binary columns to bool type\n",
    "    # for col in change_to_bool:\n",
    "    #     df[col] = df[col].astype('bool')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if tag == 'train':\n",
    "    # Method 0:\n",
    "    print('- Does not impute NaN for the remaining variables')\n",
    "    \n",
    "    # Method 1:\n",
    "    # int variables\n",
    "    #     print('Impute NaN by their median value for numerical variables')\n",
    "    #     imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "    #     for col, dtype in zip(df.columns, df.dtypes):\n",
    "    #         if dtype not in ['float', 'int', 'int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'bool', 'uint8'] or col == 'hospital_death': continue\n",
    "    #         imputer.fit(df[col].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    #         df[col] = imputer.transform(df[col].values.reshape(-1, 1))\n",
    "    #     # object variables\n",
    "    #     print('Impute NaN by their frequent value for category variables')\n",
    "    #     imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "    #     for col, dtype in zip(df.columns, df.dtypes):\n",
    "    #         if dtype not in ['object'] or col == 'hospital_death': continue\n",
    "    #         imputer.fit(df[col].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    #         df[col] = imputer.transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    '''Dealing with categorical variables\n",
    "    '''\n",
    "    \n",
    "    ### Segment age\n",
    "    ## Method 1: if age column doesn't have NaN value:\n",
    "    #     print('- Cut age into groups and add to set change_to_cat')\n",
    "    #     # df['age'] = df['age'].fillna(-1)\n",
    "    #     bins = [0, 20, 40, 60, 80, 100]\n",
    "    #     group_names = ['0-20', '20-39', '40-59', '60-79', '80-99']\n",
    "    #     df['age'] = pd.cut(df['age'], bins, labels=group_names)\n",
    "    #     if tag == 'train':\n",
    "    #         change_to_cat.append('age')\n",
    "    \n",
    "    ## Method 2: if age column has NaN value:\n",
    "    #     df['age'] = df['age'].fillna(-1)\n",
    "    #     bins = [-1, 0, 20, 40, 60, 80, 100]\n",
    "    #     group_names = ['NANN', '0-20', '20-39', '40-59', '60-79', '80-99']\n",
    "    #     df['age'] = pd.cut(df['age'], bins, labels=group_names)\n",
    "    \n",
    "    \n",
    "    ## Method 1: if column doesn't have NaN value:\n",
    "    #     print('- Change the following list into category type: ')\n",
    "    #     print(*change_to_cat)\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].astype('category')\n",
    "    \n",
    "    ## Method 2: if column has NaN value:\n",
    "    # Change factor type to category and insert NANN category\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].fillna('NANN')\n",
    "    #         df[col] = df[col].astype('category')\n",
    "    \n",
    "    \n",
    "    '''Change categorical data into different form\n",
    "    '''\n",
    "    for col in change_to_cat:\n",
    "        if (df[col].dtype == 'O') | (col=='age'):\n",
    "            df[col] = df[col].astype('object')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(-9999)\n",
    "            df[col] = df[col].astype('str').str.replace('.','')\n",
    "            df[col] = df[col].replace('-9999', np.nan) \n",
    "    \n",
    "    ## if column has NaN value:\n",
    "    # Change factor type to category and insert NANN category\n",
    "    for col in change_to_cat:\n",
    "        df[col] = df[col].fillna('NANN')\n",
    "    \n",
    "    ### Method 0:\n",
    "    #     print('- Remain categorical variables as object')\n",
    "            \n",
    "    ### Method 1: Transforming into dummy variables\n",
    "    #     print('- Change categorical data into dummies')       \n",
    "    #     df = pd.get_dummies(df, columns=change_to_cat, dummy_na=True, drop_first=False)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Checking for columns with only one value\n",
    "    print('- Drop the following columns due to containing only one value: ')\n",
    "    if tag=='train':\n",
    "        i = 0\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique() == 1:\n",
    "                excludeColumns.add(col)\n",
    "                if i==0:\n",
    "                    print(\"=> [\"+col, end=', ')\n",
    "                else:\n",
    "                    print(col, end=', ')\n",
    "                i+=1\n",
    "        print(']')\n",
    "#         print('- Drop the following columns due to containing only one value: ')\n",
    "#         for col in df.columns:\n",
    "#             if df[col].nunique() == 1:\n",
    "#                 excludeColumns.add(col)\n",
    "#                 print(col, end=', ')\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Method 1-2: Transforming into dummy variables\n",
    "    #     print('- Change categorical data into dummies, keep NaN as a category and drop the first category')\n",
    "    #     df = pd.get_dummies(df, dummy_na=True, drop_first=False)  \n",
    "\n",
    "    #     # Checking for columns with only one value\n",
    "    #     if tag == 'train':\n",
    "    #         print('- Drop the following columns due to containing only one value')\n",
    "    #         for col in df.columns:\n",
    "    #             if df[col].nunique() == 1:\n",
    "    #                 excludeColumns.add(col)\n",
    "    #                 print(col, end=', ')\n",
    "    #     print()                \n",
    "\n",
    "    ### Method 2: Encoding into integers\n",
    "    #     print('- Change factor variables to Label Encoding')\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].astype('str')\n",
    "    \n",
    "    \n",
    "    #     label_encoder = LabelEncoder()\n",
    "    #     label_class_train = {}\n",
    "    #     for col in change_to_cat:\n",
    "    #         label_encoder = label_encoder.fit(df[col])\n",
    "    #         df[col] = label_encoder.transform(df[col])\n",
    "    #         label_class_train[col] = list(label_encoder.classes_)\n",
    "    \n",
    "    ### Method 3: One hot encoding\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    '''Rename columns by replacing space into underscore\n",
    "    '''\n",
    "    # in order to avoid plot error\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    \n",
    "       \n",
    "    '''\n",
    "    Imbalance dataset:\n",
    "    '''\n",
    "    ## Method 0: Do nothing with sparse data set\n",
    "    print('- Does NOT balance sparse data set')\n",
    "    ## Method 1: Balance sparse data set by duplicating death data\n",
    "    #     if tag == 'train':\n",
    "    #         print('- Balance sparse data set by duplicating death data')\n",
    "    #         #print(len(df[df.hospital_death==1])/len(df))\n",
    "    #         #print('before:')\n",
    "    #         #print(train.hospital_death.value_counts())\n",
    "    #         # Separate majority and minority classes\n",
    "    #         df_majority = df[df.hospital_death==0]\n",
    "    #         df_minority = df[df.hospital_death==1]\n",
    "\n",
    "    #         # Resampling the minority levels to match the majority level\n",
    "    #         # Upsample minority class\n",
    "    #         df_minority_upsampled = resample(df_minority, \n",
    "    #                                          replace=True,     # sample with replacement\n",
    "    #                                          n_samples=df.hospital_death.value_counts()[0],    # to match majority class\n",
    "    #                                          random_state= 2020) # reproducible results\n",
    "    #         # Combine majority class with upsampled minority class\n",
    "    #         df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    #         # Display new class counts\n",
    "    #         #print('after:')\n",
    "    #         #print(train.hospital_death.value_counts())\n",
    "\n",
    "    \n",
    "    '''Scalling data\n",
    "    '''\n",
    "    #     print('- Scalling data by MinMaxScaler')\n",
    "    #     scaler = MinMaxScaler()\n",
    "    #     cols_to_scale = list(set(df.columns).difference(set(change_to_cat)).difference(set(excludeColumns)))\n",
    "    #     df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    if tag == 'train':\n",
    "        return df, excludeColumns, change_to_cat, new_d1_h1_cols\n",
    "    else:\n",
    "        return df, new_d1_h1_cols\n",
    "\n",
    "\n",
    "###　Define lists of columns to be transformed\n",
    "tag='train+test'\n",
    "# tag = ''\n",
    "# error_count = 0\n",
    "# while ((len(tag) == 0) or (tag not in ['train', 'test', 'train+test'])) and (error_count<=2):\n",
    "#     tag = input('Please type \"train\", \"test\", or \"train+test\" indicating this preprocess is for training, testing data set or both: ')\n",
    "#     error_count += 1\n",
    "if tag in ['train', 'test', 'train+test']:\n",
    "    if (tag == 'train') or (tag == 'train+test'):\n",
    "        change_to_bool = ['elective_surgery', 'readmission_status',\n",
    "                          'apache_post_operative', 'arf_apache',\n",
    "                          'gcs_unable_apache', 'intubated_apache',\n",
    "                          'ventilated_apache', 'aids', 'cirrhosis',\n",
    "                          'diabetes_mellitus', 'hepatic_failure',\n",
    "                          'immunosuppression', 'leukemia', 'lymphoma',\n",
    "                          'solid_tumor_with_metastasis'] ## columns will transform to boolin type\n",
    "        change_to_cat = ['ethnicity', 'gender', 'hospital_admit_source',\n",
    "                         'icu_admit_source', 'icu_stay_type', 'icu_type',\n",
    "                         'apache_3j_diagnosis', 'apache_2_diagnosis',\n",
    "                         'apache_3j_bodysystem', 'apache_2_bodysystem'] ## columns will transform to categorical type\n",
    "        excludeColumns = ['hospital_death', 'patient_id'] ## Predefine unwanted columns\n",
    "\n",
    "        outcome = 'hospital_death'\n",
    "\n",
    "    ### Data preprocessing\n",
    "    # train = pd.read_csv(\"../Dataset/training_v2.csv\",  index_col = 'encounter_id')\n",
    "    # test = pd.read_csv(\"../Dataset/unlabeled.csv\",  index_col = 'encounter_id')\n",
    "\n",
    "    if tag == 'train':\n",
    "        train, excludeColumns, change_to_cat, new_d1_h1_cols = data_processing(import_data('training_v2.csv'), pd.DataFrame(), change_to_bool, change_to_cat, set(excludeColumns), 'train')\n",
    "    elif tag == 'test':\n",
    "        test, new_d1_h1_cols_test = data_processing(import_data('unlabeled.csv'), import_data('training_v2.csv'), change_to_bool, change_to_cat, excludeColumns, 'test')\n",
    "    elif tag == 'train+test':\n",
    "        train, excludeColumns, change_to_cat, new_d1_h1_cols = data_processing(import_data('training_v2.csv'), pd.DataFrame(), change_to_bool, change_to_cat, set(excludeColumns), 'train')\n",
    "        with io.capture_output() as captured:\n",
    "            test, new_d1_h1_cols_test = data_processing(import_data('unlabeled.csv'), import_data('training_v2.csv'), change_to_bool, change_to_cat, excludeColumns, 'test')\n",
    "        # train, ohe, label_encoders = OHE(train, OneHotEncoder(), change_to_cat, 'train', {})\n",
    "        # test = OHE(test, ohe, change_to_cat, 'test', label_encoders)\n",
    "    else:\n",
    "        print('Wrong input')\n",
    "else:\n",
    "    print('End of program due to wrong input!')\n",
    "    del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Change factor variables to Label Encoding\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding version\n",
    "# %run \"data_processing.ipynb\"\n",
    "\n",
    "print('- Change factor variables to Label Encoding')\n",
    "train_len = len(train)\n",
    "df = pd.concat(objs = [train, test], axis = 0)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_class = {}\n",
    "for col in change_to_cat:\n",
    "    label_encoder = label_encoder.fit(df[col])\n",
    "    df[col] = label_encoder.transform(df[col])\n",
    "    label_class[col] = list(label_encoder.classes_)\n",
    "\n",
    "# save model to file\n",
    "pickle.dump(label_class, open('label_class_v1', \"wb\"))\n",
    "\n",
    "df[change_to_cat] = df[change_to_cat].apply(lambda x: x.astype('category'), axis=0)\n",
    "\n",
    "train = copy.copy(df[:train_len])\n",
    "test = copy.copy(df[train_len:])\n",
    "\n",
    "### Generate predictors and outcome variable for model usage\n",
    "predictors = [s for s in train.columns if s not in excludeColumns]\n",
    "outcome = 'hospital_death'\n",
    "X = train[predictors]\n",
    "y = train[outcome]\n",
    "\n",
    "X_test = test[predictors]\n",
    "\n",
    "### Split dataset into training and testing\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state = 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelFile = \"xgb_1.pickle\"\n",
    "gsFile = 'xgb_gs_1.pickle'\n",
    "LogName = 'xgb_1.log'\n",
    "\n",
    "params = {\n",
    "    'objective'         :'binary:logistic',\n",
    "    'n_estimators'      : 20000  , #default = 100\n",
    "    'random_state'      : 2020  ,\n",
    "    'seed'              : 2020  ,  \n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "params_gs = {\n",
    "    'learning_rate'     : [0.1, 0.01, 0.001]  , #default = 0.1  # typical range: 0.01 - 0.3\n",
    "    'max_depth'         : np.linspace(3,10,8,endpoint=True)     , #default = 3\n",
    "    'subsample'         : [0.8, 0.9, 1]   , #default = 1\n",
    "    'colsample_bytree'  : np.linspace(0.3,0.8,6,endpoint=True), #default = 1  # many columns: 0.3 - 0.8 , a few columns: 0.8 - 1\n",
    "    'gamma'             : [0, 1, 5]\n",
    "}\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":100, \n",
    "            \"eval_metric\" : [\"error\", 'auc']\n",
    "           }\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', \n",
    "           'Accuracy': make_scorer(accuracy_score), \n",
    "           'neg_log_loss':make_scorer(log_loss)}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(model, \n",
    "                  param_grid=params_gs, \n",
    "                  cv = 5, \n",
    "                  verbose=1,\n",
    "                  scoring=scoring, \n",
    "                  refit='AUC', \n",
    "                  return_train_score=True,\n",
    "                  n_jobs=-1\n",
    "                 )\n",
    "\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "%time gs.fit(X, y, **fit_params)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "# save model to file\n",
    "pickle.dump(gs, open(gsFile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogName = 'xgb_cv_1.log'\n",
    "\n",
    "cvFile = \"xgb_cv_1.pickle\"\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label = y)\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "%time cv = xgb.cv(gs.best_params_, \\\n",
    "                  dtrain, \\\n",
    "                  num_boost_round=50000, \\\n",
    "                  nfold=5, \\\n",
    "                  metrics=['auc'],\\\n",
    "                  early_stopping_rounds=100,\\\n",
    "                  stratified=True, \\\n",
    "                  seed=2020, \\\n",
    "                  verbose_eval=100)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "print('Best number of num_boost_round = {}'.format(cv.shape[0]))\n",
    "\n",
    "pickle.dump(cv, open(cvFile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitFile = 'submit_xgb_1.csv'\n",
    "LogName = 'xgb_submit_1.log'\n",
    "\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "model = xgb.XGBClassifier(**gs.best_params_,\n",
    "                         n_estimator = (cv.shape[0]-1),\n",
    "                         early_stopping_rounds=100,\\\n",
    "                         )\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "%time model.fit(X_train, y_train, \\\n",
    "                eval_metric=[\"error\", 'auc'], \\\n",
    "                eval_set=eval_set, \\\n",
    "                verbose=100)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "########## prediction of unlabeled data #############\n",
    "\n",
    "y_pred_unlabeled = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "data_to_submit = pd.DataFrame({\n",
    "    'encounter_id':X_test.index,\n",
    "    'hospital_death':y_pred_unlabeled\n",
    "})\n",
    "\n",
    "data_to_submit.to_csv(submitFile, index = False)\n",
    "\n",
    "data_to_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    '''create a dataframe and optimize its memory usage'''\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col = 'encounter_id')\n",
    "    #     df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def hospital_source_observation(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe without observation and update it with the most logical guess.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    #Fill missing values in the hospital admit source with ICU admit source\n",
    "    df_hos_NA = df[df['hospital_admit_source'].isna()==True]    \n",
    "    \n",
    "    #for the rows with missing value in the training data set's hospital admit source\n",
    "    for i in df_hos_NA.index:\n",
    "        \n",
    "        #if the icu amdit source is Accident & Emergency\n",
    "        if df.loc[i,'icu_admit_source']=='Accident & Emergency':\n",
    "            #hospital admit source is filled with Emergency Department\n",
    "            df.loc[i,'hospital_admit_source'] = 'Emergency Department'\n",
    "        \n",
    "        #if the icu admit source is Operating Room/Recovery\n",
    "        elif df.loc[i,'icu_admit_source'] == 'Operating Room / Recovery':\n",
    "            #if there is post operation, fill it with Operating Room\n",
    "            if df.loc[i,'apache_post_operative'] == 1:\n",
    "                df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "            #if not, fill it with Recovery Room\n",
    "            else:\n",
    "                df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "        #Fill the rest of the hospital admit source with ICU admit source\n",
    "        else:\n",
    "            df.loc[i,'hospital_admit_source'] = df.loc[i,'icu_admit_source']\n",
    "                \n",
    "    observation = df[df['hospital_admit_source']=='Observation']\n",
    "    \n",
    "    for i in observation.index:\n",
    "        \n",
    "        if df.loc[i,'icu_admit_source']!='Floor':\n",
    "            if df.loc[i,'icu_admit_source']=='Operating Room / Recovery':\n",
    "                if df.loc[i,'apache_post_operative'] == 0:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "                else:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "         \n",
    "            else:\n",
    "                df['icu_type'] = df['icu_type'].str.lower()\n",
    "                if df.loc[i,'icu_type'].find('s')!= -1:\n",
    "                    if df.loc[i,'apache_post_operative'] == 0:\n",
    "                        df.loc[i,'hospital_admit_source'] = 'Operating Room'\n",
    "                    else:\n",
    "                        df.loc[i,'hospital_admit_source'] = 'Recovery Room'\n",
    "                else:\n",
    "                    df.loc[i,'hospital_admit_source'] = 'Emergency Department'\n",
    "                        \n",
    "        else:\n",
    "            df.loc[i,'hospital_admit_source'] = df.loc[i,'icu_admit_source']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def assess_NA(data):\n",
    "    \"\"\"\n",
    "    Returns a dataframe denoting the total number of NA values and the percentage of NA values in each column.\n",
    "    The column names are notes on the index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    \"\"\"\n",
    "    #pandas series denoting features and the sum of their null values\n",
    "    null_sum = data.isnull().sum()\n",
    "    \n",
    "    #instantiate columns for missing data\n",
    "    total = null_sum\n",
    "    percent = (((null_sum/len(data.index))*100).round(2))\n",
    "    \n",
    "    #concatenate along the columns to create the complete dataframe\n",
    "    train_NA = pd.concat([total,percent],axis=1,keys=['Number of NA','Percent NA'])\n",
    "    \n",
    "    #drop rows that don't have any missing data\n",
    "    train_NA = train_NA[(train_NA.T !=0).any()].sort_values(ascending=False, by='Percent NA')\n",
    "    \n",
    "    return train_NA\n",
    "\n",
    "\n",
    "def data_processing(df, train, change_to_bool, change_to_cat, excludeColumns, tag):\n",
    "    \n",
    "    ## Drop outcome variable\n",
    "    #     if tag == 'train':\n",
    "    #         df = df.drop(columns=['hospital_death'])\n",
    "                \n",
    "    '''Revise certain Variables\n",
    "    '''\n",
    "    print('- Revise hospital_admit_source, and apache_2_bodysystem')#pre_icu_los_days\n",
    "    # Modify negative value to positive of pre_icu_los_days\n",
    "    # df['pre_icu_los_days'] = abs(df['pre_icu_los_days'])\n",
    "    # Rename some categorical variable's list name\n",
    "    df['hospital_admit_source'] = df['hospital_admit_source'].replace(\n",
    "        {'Other ICU': 'ICU', \n",
    "         'ICU to SDU':'SDU', \n",
    "         'Step-Down Unit (SDU)': 'SDU',\n",
    "         'Other':'Other Hospital', \n",
    "         'Observation': 'Recovery Room',\n",
    "         'Acute Care/Floor': 'Acute Care'})\n",
    "    df['apache_2_bodysystem'] = df['apache_2_bodysystem'].replace(\n",
    "        {'Undefined diagnoses': 'Undefined Diagnoses'}\n",
    "    )\n",
    "    # print('- Change -1 to nan for apache_4a_hospital_death_prob and apache_4a_icu_death_prob')\n",
    "    # Modify negative value of apache_4a_hospital_death_prob to nan\n",
    "    #     df.loc[df['apache_4a_hospital_death_prob'] == -1, 'apache_4a_hospital_death_prob'] = np.nan\n",
    "    #     df.loc[df['apache_4a_icu_death_prob'] == -1, 'apache_4a_icu_death_prob'] = np.nan\n",
    "    \n",
    "    ### Deal with d1, h1\n",
    "    new_d1_h1_cols = []\n",
    "    \n",
    "    ## Method 0\n",
    "    #print('- Remain h1_d1 as original value')\n",
    "    \n",
    "    ## method 1\n",
    "    ## Change all d1/h1 into booline by assigning 1 to those with values and 0 to those with NaN\n",
    "    #     d1_cols = [col for col in df.columns if (col.find('d1_') != -1)]\n",
    "    #     h1_cols = [col for col in df.columns if (col.find('h1_') != -1)]    \n",
    "    #     for col in d1_cols:\n",
    "    #         df[col] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    #     for col in h1_cols:\n",
    "    #         df[col] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    \n",
    "    ## method 1-2\n",
    "    ## Creat booline columns for d1/h1 by assigning 0 to those with values and 1 to those with NaN\n",
    "    #     print('- Creat booline columns for d1/h1')\n",
    "    #     d1_cols = [col for col in df.columns if (col.find('d1_') != -1)]\n",
    "    #     h1_cols = [col for col in df.columns if (col.find('h1_') != -1)]    \n",
    "    #     for col in d1_cols:\n",
    "    #         df[col+'_bool'] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    #     for col in h1_cols:\n",
    "    #         df[col+'_bool'] = df[col].apply(lambda x: 1 if math.isnan(x) else 0)\n",
    "    \n",
    "    ## method 2\n",
    "    #     print('- Create columns of combination of h1_d1_min_max: ')\n",
    "    #     # Add new features by aggrigating d1/h1/and their min/max\n",
    "    #     d1_cols_min = [col for col in df.columns \n",
    "    #                if (col.find('d1_') != -1 and col.find('_min') != -1)]\n",
    "    #     d1_cols_max = [col for col in df.columns \n",
    "    #                if (col.find('d1_') != -1 and col.find('_max') != -1)]\n",
    "    #     h1_cols_max = [col for col in df.columns \n",
    "    #            if (col.find('h1_') != -1 and col.find('_max') != -1)]\n",
    "\n",
    "    #     for d1_min in d1_cols_min:\n",
    "    #         d1_max = d1_min.replace('_min', '_max')\n",
    "    #         h1_min = d1_min.replace('d1', 'h1')\n",
    "    #         h1_max = h1_min.replace('_min', '_max')\n",
    "\n",
    "    #         df['range_' + d1_min.strip('_min')] = df[d1_max] - df[d1_min]; new_d1_h1_cols.append(df['range_' + d1_min.strip('_min')])\n",
    "    #         df['range_' + h1_min.strip('_min')] = df[h1_max] - df[h1_min]; new_d1_h1_cols.append(df['range_' + h1_min.strip('_min')])\n",
    "\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMin'] = df[d1_min] - df[h1_min]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMin'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMax'] = df[d1_min] - df[h1_max]; new_d1_h1_cols.append( df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MinMax'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMin'] = df[d1_max] - df[h1_min]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMin'])\n",
    "    #         df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMax'] = df[d1_max] - df[h1_max]; new_d1_h1_cols.append(df['d1_h1_' + d1_min.strip('d1_').strip('_min') + '_MaxMax'])\n",
    "\n",
    "    #     print('  There are %d columns adding in' % (len(new_d1_h1_cols)))    \n",
    "\n",
    "    #     ## Record the max columns for all d1 and h1 for future dropping list\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns = set(excludeColumns).union(d1_cols_max).union(h1_cols_max)\n",
    "\n",
    "    ### hospital_id & icu_id\n",
    "    ## Method 1\n",
    "    #     print('- Include hospital_id and icu_id and remain as original value')\n",
    "    ## Method 2\n",
    "    print('- Transfer icu_id and hopstipal_id to categorical data')\n",
    "    if tag=='train':\n",
    "        change_to_cat.append('icu_id')\n",
    "        change_to_cat.append('hospital_id')\n",
    "    ## Method 2.2\n",
    "    #     print('Transfer icu_id and to categorical data and drop hopstipal_id')\n",
    "    #     change_to_cat.append('icu_id')\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('apache_3j_diagnosis')\n",
    "    ## Method 3\n",
    "    #     print('- Exclude columns patient_id, icu_id, hospital_id, and apache_3j_diagnosis')\n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('icu_id')\n",
    "    #         excludeColumns.add('hospital_id')\n",
    "    #         excludeColumns.add('patient_id')\n",
    "    #         excludeColumns.add('apache_3j_diagnosis')\n",
    "    \n",
    "\n",
    "    ''' Drop certain columns by some criteria\n",
    "    '''\n",
    "    ### Exclude collinear (highly correlated) features \n",
    "    ## by dropping columns with correlations above threshold\n",
    "    if tag == 'train':\n",
    "        threshold = 0.9 # Threshold for removing correlated variables\n",
    "        corr_matrix = df.drop(columns='hospital_death').corr().abs() # Absolute value correlation matrix\n",
    "        # corr_matrix = df.corr().abs() # Absolute value correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) # Upper triangle of correlations\n",
    "        to_drop = set( [column for column in upper.columns if any(upper[column] > threshold)] )\n",
    "        print('- There are %d columns to remove due to high correlation with threshold=0.9.' % (len(to_drop)))\n",
    "\n",
    "        # record these columns into the list, excludeColumns\n",
    "        excludeColumns = excludeColumns.union(to_drop)\n",
    "\n",
    "#         ## Exclude h1_xxx_max and d1_xxx_max features \n",
    "#         df = df[df.columns.drop(list(df.filter(regex='_max')))]\n",
    "    \n",
    "    \n",
    "    '''Deal with missing values\n",
    "    '''\n",
    "    ### fillna in height/weight/bmi\n",
    "    if tag=='train':\n",
    "        print('- Fill NaN of age, height, weight, and bmi')\n",
    "        df['age'] = df['age'].fillna(df['age'].median())\n",
    "        df['height'] = df.groupby(['ethnicity', 'gender'])['height'].apply(lambda x: x.fillna(x.median()))  \n",
    "        df['weight'] = df.groupby(['ethnicity', 'gender', 'height'])['weight'].apply(lambda x: x.fillna(x.median()))    \n",
    "        # print('Drop height and weight, remain only column bmi ')    \n",
    "    #     if tag == 'train':\n",
    "    #         excludeColumns.add('height')\n",
    "    #         excludeColumns.add('weight')\n",
    "    else:\n",
    "        print('- Fill NaN of age, height, weight, and bmi by values from training data set')\n",
    "        df['age'] = df['age'].fillna(train['age'].median())\n",
    "        df['height'] = df['height'].fillna(train.groupby(['ethnicity', 'gender'])['height'].transform('median'))\n",
    "        df['weight'] = df['weight'].fillna(train.groupby(['ethnicity', 'gender', 'height'])['weight'].transform('median'))\n",
    "    \n",
    "    df['bmi'] = df.apply( lambda x: x['weight']/(0.01 * x['height'])**2 if np.isnan(x['bmi']) else x['bmi'], axis=1)\n",
    "    \n",
    "    ### fillna for hospital_admit_source\n",
    "    print('- Fill NaN of hospital_admit_source by using icu_type')\n",
    "    df = hospital_source_observation(df)\n",
    "\n",
    "    print('- Fill NaN of hospital_admit_source by using most frequent value in icu_type')    \n",
    "    imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "    imputer.fit(df['hospital_admit_source'].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    if tag == 'train':\n",
    "        df['hospital_admit_source'] = imputer.transform(df['hospital_admit_source'].values.reshape(-1, 1))\n",
    "    else:\n",
    "        most_freq = train['hospital_admit_source'].value_counts().keys()[0]\n",
    "        df['hospital_admit_source'] = df['hospital_admit_source'].fillna(most_freq)\n",
    "\n",
    "    ### fillna for apache_4a_hospital_death_prob and apache_4a_icu_death_prob\n",
    "    print('- Fill NaN of apache_4a_hospital_death_prob and apache_4a_icu_death_prob by each other')\n",
    "    df['apache_4a_hospital_death_prob'\n",
    "        ] = pd.DataFrame(df.loc[:,['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']\n",
    "                                 ].apply(lambda x: x['apache_4a_icu_death_prob'] if np.isnan(x['apache_4a_hospital_death_prob']) \\\n",
    "                                                                                 else x['apache_4a_hospital_death_prob'], \n",
    "                                         axis=1))\n",
    "    df['apache_4a_icu_death_prob'\n",
    "        ] = pd.DataFrame(df.loc[:,['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']\n",
    "                                 ].apply(lambda x: x['apache_4a_hospital_death_prob'] if np.isnan(x['apache_4a_icu_death_prob']) \\\n",
    "                                                                                      else x['apache_4a_icu_death_prob'], \n",
    "                                         axis=1))\n",
    "\n",
    "    ### fillna for all numerical and object variables using '9999'\n",
    "    #     print('- Fill NaN as \"9999\" for the following object columns:')\n",
    "    #     i = 0\n",
    "    #     for col in change_to_cat:\n",
    "    #         if i==0:\n",
    "    #             print(\"=> [\"+col, end=', ')\n",
    "    #         elif i!=len(change_to_cat)-1:\n",
    "    #             print(col, end=', ')\n",
    "    #         else:\n",
    "    #             print(col+']')\n",
    "    #         i+=1\n",
    "    #     for col in change_to_cat:\n",
    "    #         if col != 'age':\n",
    "    #             df[col] = df[col].fillna('9999')\n",
    "    #             df[col] = df[col].astype('str')\n",
    "\n",
    "    #     print('- Fill NaN as \"9999\" for the remaining numeric columns')\n",
    "    #     for col in set(df.columns).difference(set(change_to_cat)):\n",
    "    #         df[col] = df[col].fillna(9999)\n",
    "    \n",
    "    ### Binary variables\n",
    "    ## Method 1: make NaN as a new group NaNN: \n",
    "    # which represents a patient either doesn't need to fill the form,\n",
    "    # or they couldn't fill the form\n",
    "    # print('Create an NANN category for boolean variables')\n",
    "    # for col in change_to_bool:\n",
    "    #     df[col] = df[col].fillna('NANN')\n",
    "    #     df[col] = df[col].astype('category')\n",
    "    ## Method 2: change NaN --> 0\n",
    "    #     for col in change_to_bool:\n",
    "    #         df[col] = df[col].fillna(0)\n",
    "    \n",
    "    ## change binary columns to bool type\n",
    "    # for col in change_to_bool:\n",
    "    #     df[col] = df[col].astype('bool')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if tag == 'train':\n",
    "    # Method 0:\n",
    "    print('- Does not impute NaN for the remaining variables')\n",
    "    \n",
    "    # Method 1:\n",
    "    # int variables\n",
    "    #     print('Impute NaN by their median value for numerical variables')\n",
    "    #     imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "    #     for col, dtype in zip(df.columns, df.dtypes):\n",
    "    #         if dtype not in ['float', 'int', 'int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'bool', 'uint8'] or col == 'hospital_death': continue\n",
    "    #         imputer.fit(df[col].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    #         df[col] = imputer.transform(df[col].values.reshape(-1, 1))\n",
    "    #     # object variables\n",
    "    #     print('Impute NaN by their frequent value for category variables')\n",
    "    #     imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "    #     for col, dtype in zip(df.columns, df.dtypes):\n",
    "    #         if dtype not in ['object'] or col == 'hospital_death': continue\n",
    "    #         imputer.fit(df[col].values.reshape(-1, 1)) # Try it only when get 1-D, 2-D array error\n",
    "    #         df[col] = imputer.transform(df[col].values.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    \n",
    "    '''Dealing with categorical variables\n",
    "    '''\n",
    "    \n",
    "    ### Segment age\n",
    "    ## Method 1: if age column doesn't have NaN value:\n",
    "    #     print('- Cut age into groups and add to set change_to_cat')\n",
    "    #     # df['age'] = df['age'].fillna(-1)\n",
    "    #     bins = [0, 20, 40, 60, 80, 100]\n",
    "    #     group_names = ['0-20', '20-39', '40-59', '60-79', '80-99']\n",
    "    #     df['age'] = pd.cut(df['age'], bins, labels=group_names)\n",
    "    #     if tag == 'train':\n",
    "    #         change_to_cat.append('age')\n",
    "    \n",
    "    ## Method 2: if age column has NaN value:\n",
    "    #     df['age'] = df['age'].fillna(-1)\n",
    "    #     bins = [-1, 0, 20, 40, 60, 80, 100]\n",
    "    #     group_names = ['NANN', '0-20', '20-39', '40-59', '60-79', '80-99']\n",
    "    #     df['age'] = pd.cut(df['age'], bins, labels=group_names)\n",
    "    \n",
    "    \n",
    "    ## Method 1: if column doesn't have NaN value:\n",
    "    #     print('- Change the following list into category type: ')\n",
    "    #     print(*change_to_cat)\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].astype('category')\n",
    "    \n",
    "    ## Method 2: if column has NaN value:\n",
    "    # Change factor type to category and insert NANN category\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].fillna('NANN')\n",
    "    #         df[col] = df[col].astype('category')\n",
    "    \n",
    "    \n",
    "    '''Change categorical data into different form\n",
    "    '''\n",
    "    for col in change_to_cat:\n",
    "        if (df[col].dtype == 'O') | (col=='age'):\n",
    "            df[col] = df[col].astype('object')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(-9999)\n",
    "            df[col] = df[col].astype('str').str.replace('.','')\n",
    "            df[col] = df[col].replace('-9999', np.nan) \n",
    "    \n",
    "    ## if column has NaN value:\n",
    "    # Change factor type to category and insert NANN category\n",
    "    for col in change_to_cat:\n",
    "        df[col] = df[col].fillna('NANN')\n",
    "    \n",
    "    ### Method 0:\n",
    "    #     print('- Remain categorical variables as object')\n",
    "            \n",
    "    ### Method 1: Transforming into dummy variables\n",
    "    print('- Change categorical data into dummies')       \n",
    "    df = pd.get_dummies(df, columns=change_to_cat, dummy_na=True, drop_first=False)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Checking for columns with only one value\n",
    "    print('- Drop the following columns due to containing only one value: ')\n",
    "    if tag=='train':\n",
    "        i = 0\n",
    "        for col in df.columns:\n",
    "            if df[col].nunique() == 1:\n",
    "                excludeColumns.add(col)\n",
    "                if i==0:\n",
    "                    print(\"=> [\"+col, end=', ')\n",
    "                else:\n",
    "                    print(col, end=', ')\n",
    "                i+=1\n",
    "        print(']')\n",
    "#         print('- Drop the following columns due to containing only one value: ')\n",
    "#         for col in df.columns:\n",
    "#             if df[col].nunique() == 1:\n",
    "#                 excludeColumns.add(col)\n",
    "#                 print(col, end=', ')\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Method 1-2: Transforming into dummy variables\n",
    "    #     print('- Change categorical data into dummies, keep NaN as a category and drop the first category')\n",
    "    #     df = pd.get_dummies(df, dummy_na=True, drop_first=False)  \n",
    "\n",
    "    #     # Checking for columns with only one value\n",
    "    #     if tag == 'train':\n",
    "    #         print('- Drop the following columns due to containing only one value')\n",
    "    #         for col in df.columns:\n",
    "    #             if df[col].nunique() == 1:\n",
    "    #                 excludeColumns.add(col)\n",
    "    #                 print(col, end=', ')\n",
    "    #     print()                \n",
    "\n",
    "    ### Method 2: Encoding into integers\n",
    "    #     print('- Change factor variables to Label Encoding')\n",
    "    #     for col in change_to_cat:\n",
    "    #         df[col] = df[col].astype('str')\n",
    "    \n",
    "    \n",
    "    #     label_encoder = LabelEncoder()\n",
    "    #     label_class_train = {}\n",
    "    #     for col in change_to_cat:\n",
    "    #         label_encoder = label_encoder.fit(df[col])\n",
    "    #         df[col] = label_encoder.transform(df[col])\n",
    "    #         label_class_train[col] = list(label_encoder.classes_)\n",
    "    \n",
    "    ### Method 3: One hot encoding\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    '''Rename columns by replacing space into underscore\n",
    "    '''\n",
    "    # in order to avoid plot error\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "    \n",
    "       \n",
    "    '''\n",
    "    Imbalance dataset:\n",
    "    '''\n",
    "    ## Method 0: Do nothing with sparse data set\n",
    "    print('- Does NOT balance sparse data set')\n",
    "    ## Method 1: Balance sparse data set by duplicating death data\n",
    "    #     if tag == 'train':\n",
    "    #         print('- Balance sparse data set by duplicating death data')\n",
    "    #         #print(len(df[df.hospital_death==1])/len(df))\n",
    "    #         #print('before:')\n",
    "    #         #print(train.hospital_death.value_counts())\n",
    "    #         # Separate majority and minority classes\n",
    "    #         df_majority = df[df.hospital_death==0]\n",
    "    #         df_minority = df[df.hospital_death==1]\n",
    "\n",
    "    #         # Resampling the minority levels to match the majority level\n",
    "    #         # Upsample minority class\n",
    "    #         df_minority_upsampled = resample(df_minority, \n",
    "    #                                          replace=True,     # sample with replacement\n",
    "    #                                          n_samples=df.hospital_death.value_counts()[0],    # to match majority class\n",
    "    #                                          random_state= 2020) # reproducible results\n",
    "    #         # Combine majority class with upsampled minority class\n",
    "    #         df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    #         # Display new class counts\n",
    "    #         #print('after:')\n",
    "    #         #print(train.hospital_death.value_counts())\n",
    "\n",
    "    \n",
    "    '''Scalling data\n",
    "    '''\n",
    "    #     print('- Scalling data by MinMaxScaler')\n",
    "    #     scaler = MinMaxScaler()\n",
    "    #     cols_to_scale = list(set(df.columns).difference(set(change_to_cat)).difference(set(excludeColumns)))\n",
    "    #     df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    if tag == 'train':\n",
    "        return df, excludeColumns, change_to_cat, new_d1_h1_cols\n",
    "    else:\n",
    "        return df, new_d1_h1_cols\n",
    "\n",
    "\n",
    "###　Define lists of columns to be transformed\n",
    "tag='train+test'\n",
    "# tag = ''\n",
    "# error_count = 0\n",
    "# while ((len(tag) == 0) or (tag not in ['train', 'test', 'train+test'])) and (error_count<=2):\n",
    "#     tag = input('Please type \"train\", \"test\", or \"train+test\" indicating this preprocess is for training, testing data set or both: ')\n",
    "#     error_count += 1\n",
    "if tag in ['train', 'test', 'train+test']:\n",
    "    if (tag == 'train') or (tag == 'train+test'):\n",
    "        change_to_bool = ['elective_surgery', 'readmission_status',\n",
    "                          'apache_post_operative', 'arf_apache',\n",
    "                          'gcs_unable_apache', 'intubated_apache',\n",
    "                          'ventilated_apache', 'aids', 'cirrhosis',\n",
    "                          'diabetes_mellitus', 'hepatic_failure',\n",
    "                          'immunosuppression', 'leukemia', 'lymphoma',\n",
    "                          'solid_tumor_with_metastasis'] ## columns will transform to boolin type\n",
    "        change_to_cat = ['ethnicity', 'gender', 'hospital_admit_source',\n",
    "                         'icu_admit_source', 'icu_stay_type', 'icu_type',\n",
    "                         'apache_3j_diagnosis', 'apache_2_diagnosis',\n",
    "                         'apache_3j_bodysystem', 'apache_2_bodysystem'] ## columns will transform to categorical type\n",
    "        excludeColumns = ['hospital_death', 'patient_id'] ## Predefine unwanted columns\n",
    "\n",
    "        outcome = 'hospital_death'\n",
    "\n",
    "    ### Data preprocessing\n",
    "    # train = pd.read_csv(\"../Dataset/training_v2.csv\",  index_col = 'encounter_id')\n",
    "    # test = pd.read_csv(\"../Dataset/unlabeled.csv\",  index_col = 'encounter_id')\n",
    "\n",
    "    if tag == 'train':\n",
    "        train, excludeColumns, change_to_cat, new_d1_h1_cols = data_processing(import_data('training_v2.csv'), pd.DataFrame(), change_to_bool, change_to_cat, set(excludeColumns), 'train')\n",
    "    elif tag == 'test':\n",
    "        test, new_d1_h1_cols_test = data_processing(import_data('unlabeled.csv'), import_data('training_v2.csv'), change_to_bool, change_to_cat, excludeColumns, 'test')\n",
    "    elif tag == 'train+test':\n",
    "        train, excludeColumns, change_to_cat, new_d1_h1_cols = data_processing(import_data('training_v2.csv'), pd.DataFrame(), change_to_bool, change_to_cat, set(excludeColumns), 'train')\n",
    "        with io.capture_output() as captured:\n",
    "            test, new_d1_h1_cols_test = data_processing(import_data('unlabeled.csv'), import_data('training_v2.csv'), change_to_bool, change_to_cat, excludeColumns, 'test')\n",
    "        # train, ohe, label_encoders = OHE(train, OneHotEncoder(), change_to_cat, 'train', {})\n",
    "        # test = OHE(test, ohe, change_to_cat, 'test', label_encoders)\n",
    "    else:\n",
    "        print('Wrong input')\n",
    "else:\n",
    "    print('End of program due to wrong input!')\n",
    "    del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Variable version\n",
    "# %run \"data_processing2.ipynb\"\n",
    "\n",
    "### Generate predictors and outcome variable for model usage\n",
    "predictors = [s for s in train.columns if s not in excludeColumns]\n",
    "outcome = 'hospital_death'\n",
    "X = train[predictors]\n",
    "y = train[outcome]\n",
    "\n",
    "### Split dataset into training and testing\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state = 2020)\n",
    "\n",
    "# X_test = test[predictors]\n",
    "cols_when_model_builds  = set(X_train.columns)\n",
    "X_test= test.copy()\n",
    "for col in set(cols_when_model_builds).difference(X_test.columns):\n",
    "    X_test[col] = np.nan\n",
    "X_test = X_test[cols_when_model_builds]\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelFile = \"xgb_1.pickle\"\n",
    "gsFile = 'xgb_gs_2.pickle'\n",
    "LogName = 'xgb_2.log'\n",
    "\n",
    "params = {\n",
    "    'objective'         :'binary:logistic',\n",
    "    'n_estimators'      : 20000  , #default = 100\n",
    "    'random_state'      : 2020  ,\n",
    "    'seed'              : 2020  ,  \n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(**params)\n",
    "\n",
    "params_gs = {\n",
    "    'learning_rate'     : [0.1, 0.01, 0.001]  , #default = 0.1  # typical range: 0.01 - 0.3\n",
    "    'max_depth'         : np.linspace(3,10,8,endpoint=True)     , #default = 3\n",
    "    'subsample'         : [0.8, 0.9, 1]   , #default = 1\n",
    "    'colsample_bytree'  : np.linspace(0.3,0.8,6,endpoint=True), #default = 1  # many columns: 0.3 - 0.8 , a few columns: 0.8 - 1\n",
    "    'gamma'             : [0, 1, 5]\n",
    "}\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":100, \n",
    "            \"eval_metric\" : [\"error\", 'auc']\n",
    "           }\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', \n",
    "           'Accuracy': make_scorer(accuracy_score), \n",
    "           'neg_log_loss':make_scorer(log_loss)}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(model, \n",
    "                  param_grid=params_gs, \n",
    "                  cv = 5, \n",
    "                  verbose=1,\n",
    "                  scoring=scoring, \n",
    "                  refit='AUC', \n",
    "                  return_train_score=True,\n",
    "                  n_jobs=-1\n",
    "                 )\n",
    "\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "%time gs.fit(X, y, **fit_params)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "# save model to file\n",
    "pickle.dump(gs, open(gsFile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogName = 'xgb_cv_2.log'\n",
    "\n",
    "cvFile = \"xgb_cv_2.pickle\"\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label = y)\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "%time cv = xgb.cv(gs.best_params_, \\\n",
    "                  dtrain, \\\n",
    "                  num_boost_round=50000, \\\n",
    "                  nfold=5, \\\n",
    "                  metrics=['auc'],\\\n",
    "                  early_stopping_rounds=100,\\\n",
    "                  stratified=True, \\\n",
    "                  seed=2020, \\\n",
    "                  verbose_eval=100)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "print('Best number of num_boost_round = {}'.format(cv.shape[0]))\n",
    "\n",
    "pickle.dump(cv, open(cvFile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.934120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.919474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.920643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0.968877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.947625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  hospital_death\n",
       "0             2        0.934120\n",
       "1             5        0.919474\n",
       "2             7        0.920643\n",
       "3             8        0.968877\n",
       "4            10        0.947625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submitFile = 'submit_xgb_2.csv'\n",
    "LogName = 'xgb_submit_2.log'\n",
    "\n",
    "\n",
    "fsock=open(LogName,'w')\n",
    "sys_old_out_put=sys.stdout\n",
    "sys.stdout=fsock\n",
    "\n",
    "model = xgb.XGBClassifier(**gs.best_params_,\n",
    "                         n_estimator = (cv.shape[0]-1),\n",
    "                         early_stopping_rounds=100,\\\n",
    "                         )\n",
    "\n",
    "eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "%time model.fit(X_train, y_train, \\\n",
    "                eval_metric=[\"error\", 'auc'], \\\n",
    "                eval_set=eval_set, \\\n",
    "                verbose=100)\n",
    "\n",
    "sys.stdout=sys_old_out_put\n",
    "fsock.close()\n",
    "\n",
    "########## prediction of unlabeled data #############\n",
    "\n",
    "y_pred_unlabeled = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "data_to_submit = pd.DataFrame({\n",
    "    'encounter_id':X_test.index,\n",
    "    'hospital_death':y_pred_unlabeled\n",
    "})\n",
    "\n",
    "data_to_submit.to_csv(submitFile, index = False)\n",
    "\n",
    "data_to_submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
